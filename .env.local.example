# ── LLM Provider ─────────────────────────────────────────────────────────────
# anthropic | openai | grok | openrouter
AI_PROVIDER=grok

# ── ROMA Mode ─────────────────────────────────────────────────────────────────
# keen   all agents use fast model    (~15–25s pipeline)
# smart  fast/smart tiers as normal   (~30–60s pipeline)  ← default
# deep   all agents use deep model    (~60–120s pipeline)
ROMA_MODE=smart

# Direct provider keys
XAI_API_KEY=
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...

# ── Model overrides (optional — defaults shown) ───────────────────────────────
# Anthropic  — fast: haiku (~2–5s) | smart: sonnet (~10–20s) | deep: opus (~20–40s)
ANTHROPIC_FAST_MODEL=claude-haiku-4-5-20251001
ANTHROPIC_SMART_MODEL=claude-sonnet-4-6
ANTHROPIC_DEEP_MODEL=claude-opus-4-6

# OpenAI     — fast: gpt-4o-mini (~3–8s) | smart: gpt-4o (~10–20s) | deep: gpt-4o or o3-mini
OPENAI_FAST_MODEL=gpt-4o-mini
OPENAI_SMART_MODEL=gpt-4o
OPENAI_DEEP_MODEL=gpt-4o

# Grok (xAI) — fast: grok-3-fast (~5–10s) | smart: grok-4-0709 (~15–30s) | deep: grok-4-0709 or grok-4-1-fast-reasoning
GROK_FAST_MODEL=grok-3-fast
GROK_SMART_MODEL=grok-4-0709
GROK_DEEP_MODEL=grok-4-0709

# OpenRouter — set AI_PROVIDER=openrouter and pick any model
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_MODEL=x-ai/grok-3          # smart tier  (default: anthropic/claude-sonnet-4-6)
OPENROUTER_FAST_MODEL=x-ai/grok-3-mini # fast tier   (default: OPENROUTER_MODEL)

# Python ROMA microservice (roma-dspy). Set to empty string to skip and use TypeScript ROMA fallback.
PYTHON_ROMA_URL=http://localhost:8001

CMC_API_KEY=
KALSHI_API_KEY=
KALSHI_PRIVATE_KEY_PATH=
